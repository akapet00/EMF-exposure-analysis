{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "from jax import vmap, jit, grad\n",
    "from jax.example_libraries import optimizers\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as e:\n",
    "    install = input(f'{e}, Do you want to install it? [Y/n]')\n",
    "    if install == 'Y':\n",
    "        import sys\n",
    "        !{sys.executable} -m pip install 'scikit-learn'\n",
    "        import sklearn\n",
    "        print(sklearn.__version__)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dosipy.utils.viz import fig_config, save_fig, set_colorblind\n",
    "from dosipy.utils.dataloader import load_antenna_el_properties\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-necklace",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'platform: {jax.lib.xla_bridge.get_backend().platform}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_colorblind()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-computed source data\n",
    "\n",
    "f = 60e9  # operating frequency of the antenna\n",
    "antenna_data = load_antenna_el_properties(f)\n",
    "Is = antenna_data.ireal.to_numpy() + antenna_data.iimag.to_numpy() * 1j\n",
    "Is = np.asarray(Is)\n",
    "xs = antenna_data.x.to_numpy()\n",
    "L = xs[-1]\n",
    "Imax = np.abs(Is).max()\n",
    "xticks = [0, L/2, L]\n",
    "xticklabels = [-round(L*1000/2, 2), 0.0, round(L*1000/2, 2)]\n",
    "xlabel = '$x$ [mm]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution\n",
    "\n",
    "yticks = [0, Imax/2, Imax]\n",
    "yticklabels = [0.0, round(Imax/2, 3), round(Imax, 3)]\n",
    "ylabel = '$I(x)$ [A]'\n",
    "\n",
    "fig_config(latex=True, text_size=20, line_width=2.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xs, np.abs(Is), 'k-', label='$|I(x)|$')\n",
    "ax.plot(xs, np.real(Is), 'k--', label='$\\Re{[I(x)]}$')\n",
    "ax.plot(xs, np.imag(Is), 'k:', label='$\\Im{[I(x)]}$')\n",
    "ax.set(xlabel=xlabel,\n",
    "       xticks=xticks,\n",
    "       xticklabels=xticklabels,\n",
    "       ylabel=ylabel,\n",
    "       yticks=[-Imax/2, *yticks],\n",
    "       yticklabels=[-round(Imax/2, 3), *yticklabels])\n",
    "ax.legend(prop={'size': 20})\n",
    "\n",
    "#fname = os.path.join('figures', 'complex_current')\n",
    "#save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diff(fn, h=1e-6):\n",
    "    \"\"\"Forward difference approximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x + h) - fn(x)) / h\n",
    "    return dfn_dx\n",
    "    \n",
    "    \n",
    "def backward_diff(fn, h=1e-6):\n",
    "    \"\"\"Backward difference approximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x) - fn(x - h)) / h\n",
    "    return dfn_dx\n",
    "\n",
    "    \n",
    "def central_diff(fn, h=1e-6):\n",
    "    \"\"\"Central difference approximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x + h) - fn(x - h)) / (2 * h)\n",
    "    return dfn_dx\n",
    "\n",
    "\n",
    "def complex_step_diff(fn, h=1e-6):\n",
    "    \"\"\"Complex-step difference approximation.\n",
    "    \n",
    "    Note: Incompatible with SciPy interpolation module.\n",
    "    \"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return np.imag(fn(x + 1j * h)) / h\n",
    "    return dfn_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadratic interpolation of current distribution function\n",
    "\n",
    "Is_fn = interp1d(xs, np.abs(Is), kind='quadratic')\n",
    "\n",
    "xs_interp = np.linspace(xs.min(), xs.max(), 961)\n",
    "Is_interp = Is_fn(xs_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean relative error\n",
    "\n",
    "error = np.abs(np.abs(Is)[1:-1] - Is_fn(xs)[1:-1]) / np.abs(Is)[1:-1]\n",
    "mean_error = np.mean(error)\n",
    "print(f'relative error = {mean_error * 100:.2e} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution as calculated and interpolated\n",
    "\n",
    "fig_config(latex=True, text_size=20, line_width=2.5, marker_size=7)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xs, np.abs(Is), 'k-', label='$|I(x)|$')\n",
    "ax.plot(xs_interp, Is_interp, 'ko', markevery=30, label='$|\\hat{I}(x)|$')\n",
    "ax.set(xlabel=xlabel,\n",
    "       xticks=xticks,\n",
    "       xticklabels=xticklabels,\n",
    "       ylabel=ylabel,\n",
    "       yticks=yticks,\n",
    "       yticklabels=yticklabels)\n",
    "ax.legend()\n",
    "\n",
    "# fname = os.path.join('figures', 'interp_current')\n",
    "# save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finite difference on computed and interpolated data\n",
    "\n",
    "dIsdxs = np.r_[(forward_diff(Is_fn)(xs[0]),\n",
    "                central_diff(Is_fn)(xs[1:-1]),\n",
    "                backward_diff(Is_fn)(xs[-1]))]\n",
    "dIsdxs_interp = np.r_[(forward_diff(Is_fn)(xs_interp[0]),\n",
    "                       central_diff(Is_fn)(xs_interp[1:-1]),\n",
    "                       backward_diff(Is_fn)(xs_interp[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize current gradient distribution as calculated and interpolated\n",
    "\n",
    "fig_config(latex=True, text_size=20, line_width=2.5, marker_size=7)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xs, dIsdxs, 'k-', label='$\\mathrm{d}|I|$/$\\mathrm{d}x$')\n",
    "ax.plot(xs_interp, dIsdxs_interp, 'ko', markevery=30, label='$\\mathrm{d}|\\hat{I}|$/$\\mathrm{d}x$')\n",
    "ax.plot([xs[1], xs[-2]], [dIsdxs[1], dIsdxs[-2]], 'ko', fillstyle='none',\n",
    "        markersize=50, markeredgewidth=2.5)\n",
    "textbox = ax.text(xs[0], -dIsdxs[1]-4, s='numerical artifacts', fontweight='bold',\n",
    "                  size=18,\n",
    "                  bbox={'facecolor': 'lightgray',\n",
    "                        'edgecolor': 'black',\n",
    "                        'alpha': 1,\n",
    "                        'pad': 5})\n",
    "ax.annotate('', xy=(xs[2], dIsdxs[5]-2),\n",
    "            xytext=(xs[11], -dIsdxs[2]),\n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.annotate('', xy=(xs[-6], dIsdxs[-2]),\n",
    "            xytext=(xs[11]+0.000925, -dIsdxs[2]-5), \n",
    "            arrowprops={'facecolor': 'black',})\n",
    "ax.set(xlabel=xlabel, ylabel=r'$\\mathrm{d}I$/$\\mathrm{d}x$ [A/m]',\n",
    "       xticks=xticks,\n",
    "       xticklabels=xticklabels)\n",
    "ax.legend(prop={'size': 18})\n",
    "\n",
    "# fname = os.path.join('figures', 'grad_interp_current')\n",
    "# save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"Initialize network parameters.\"\"\"\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    def random_layer_params(m, n, key, scale=1e-2):\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        return (scale * jax.random.normal(w_key, (n, m)),\n",
    "                scale * jax.random.normal(b_key, (n, )))\n",
    "    return [random_layer_params(m, n, key)\n",
    "            for m, n, key in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "\n",
    "def forward(params, X, scaler):\n",
    "    \"\"\"Forward pass.\"\"\"\n",
    "    output = X\n",
    "    for w, b in params[:-1]:\n",
    "        output = nn.tanh(w @ output + b)\n",
    "    w, b = params[-1]\n",
    "    output = w @ output + b\n",
    "    return output * scaler\n",
    "\n",
    "\n",
    "# vectorized mapping of network input, `X`, on `forward` function\n",
    "batch_forward = vmap(forward, in_axes=(None, 0, None))\n",
    "\n",
    "\n",
    "@jit\n",
    "def loss_fn(params, batch, scaler):\n",
    "    \"\"\"Summed square error loss function.\"\"\"\n",
    "    X, y = batch\n",
    "    y_pred = batch_forward(params, X, scaler)\n",
    "    return jnp.sum(jnp.square(y_pred - y))\n",
    "\n",
    "\n",
    "# derivative of the loss function\n",
    "grad_fn = jit(grad(loss_fn))\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(step, optim_state, batch, scaler):\n",
    "    \"\"\"Return current optimal state of the network.\"\"\"\n",
    "    params = optim_params(optim_state)\n",
    "    grads = grad_fn(params, batch, scaler)\n",
    "    optim_state = optim_update(step, grads, optim_state)\n",
    "    return optim_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-supervisor",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_norm = normalize(xs_interp, xs_interp)\n",
    "xs_data = jnp.array(xs_norm).reshape(-1, 1)\n",
    "# the following scaling is a bit weird but is needed in order to keep\n",
    "# gradients of the nn output wrt input (not parameters, but actual input) in\n",
    "# the same scale with the ones that are computed via FDM\n",
    "Is_norm = normalize(xs_interp, Is_interp)\n",
    "Is_data = jnp.array(Is_norm).reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs_data, Is_data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set network hyperparameter and train\n",
    "\n",
    "# to set the output of the nn in scale with the target data, we define scaler\n",
    "scaler = np.abs(y_train).max()\n",
    "step_size = 1e-3\n",
    "n_epochs = 10_000\n",
    "printout = int(n_epochs / 100.)\n",
    "epochs = np.arange(0, n_epochs+1, step=printout)\n",
    "batch_size = 128\n",
    "momentum_mass = 0.9  # for momentum and adagrad\n",
    "sizes = [1, 128, 256, 128, 1]\n",
    "\n",
    "num_train = X_train.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "\n",
    "def data_stream(num_train, num_batches):\n",
    "    \"\"\"Training data random generator.\"\"\"\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield X_train[batch_idx], y_train[batch_idx]\n",
    "            \n",
    "\n",
    "batches = data_stream(num_train, num_batches)\n",
    "\n",
    "optim_init, optim_update, optim_params = optimizers.adam(step_size)\n",
    "init_params = init_network_params(sizes, rng)\n",
    "optim_state = optim_init(init_params)\n",
    "itercount = itertools.count()\n",
    " \n",
    "loss_train, loss_test = [], []\n",
    "params_list = []\n",
    "start_time = time.time()\n",
    "pbar = tqdm(range(n_epochs))\n",
    "for epoch in pbar:\n",
    "    start_epoch_time = time.time()\n",
    "    for _ in range(num_batches):\n",
    "        optim_state = update(next(itercount), optim_state, next(batches), scaler)\n",
    "    epoch_duration = time.time() - start_epoch_time\n",
    "    \n",
    "    params = optim_params(optim_state)\n",
    "    if (epoch == 0) or (epoch % printout == (printout - 1)):\n",
    "        params_list.append(params)\n",
    "        curr_loss_train_val = loss_fn(params, (X_train, y_train), scaler)\n",
    "        curr_loss_test_val = loss_fn(params, (X_test, y_test), scaler)\n",
    "        loss_train.append(curr_loss_train_val)\n",
    "        loss_test.append(curr_loss_test_val)\n",
    "        pbar.set_description(f'Loss (test): {curr_loss_test_val:.4e}')\n",
    "training_duration = time.time() - start_time\n",
    "print(f'Training time: {training_duration:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training loss dynamics\n",
    "\n",
    "fig_config(latex=True, text_size=24, line_width=4)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(epochs[::10], loss_train[::10], 'k-', label='train set')\n",
    "ax.plot(epochs[::10], loss_test[::10], 'k--', label='test set')\n",
    "ax.set(xlabel='epoch', ylabel='loss', yscale='log', yticks=[1e-1, 1e1, 1e3])\n",
    "ax.legend(prop={'size': 24})\n",
    "\n",
    "# fname = os.path.join('figures', 'loss')\n",
    "# save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose params with the best performance on test set\n",
    "\n",
    "best_params_idx = loss_test.index(min(loss_test))\n",
    "params = params_list[best_params_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution as calculated and fitted\n",
    "\n",
    "Is_fit = batch_forward(params, xs_data.reshape(-1, 1), scaler)\n",
    "Is_fit_inv_norm = inv_normalize(Is_fit, xs_interp)\n",
    "\n",
    "fig_config(latex=True, text_size=20, line_width=2.5, marker_size=7)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xs_interp, Is_interp, 'k-', label='$|I(x)|$')\n",
    "ax.plot(xs_interp, Is_fit_inv_norm, 'ko', markevery=30, label='NN$(x)$')\n",
    "ax.set(xlabel=xlabel,\n",
    "       xticks=xticks,\n",
    "       xticklabels=xticklabels,\n",
    "       ylabel=ylabel,\n",
    "       yticks=yticks,\n",
    "       yticklabels=yticklabels)\n",
    "ax.legend()\n",
    "\n",
    "# fname = os.path.join('figures', 'nn_current')\n",
    "# save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Is_nn(xs):\n",
    "    \"\"\"Current value at specific location, `xs`.\n",
    "    \n",
    "    Note: This is single-value wrapper for the forward pass function.\n",
    "    \"\"\"\n",
    "    return forward(params, xs, scaler)[0]\n",
    "\n",
    "\n",
    "# derivative of the current approximation function\n",
    "grad_Is_nn = jit(vmap(grad(Is_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize current gradient distribution as calculated and fitted\n",
    "\n",
    "fig_config(latex=True, text_size=20, line_width=2.5, marker_size=7)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(xs_interp, dIsdxs_interp, 'k-', label='$\\mathrm{d}|I|$/$\\mathrm{d}x$')\n",
    "ax.plot(xs_interp, grad_Is_nn(xs_data), 'ko', markevery=30, label='$\\mathrm{d}$NN/$\\mathrm{d}x$')\n",
    "ax.plot([xs[1], xs[-2]], [dIsdxs[1], dIsdxs[-2]], 'ko', fillstyle='none',\n",
    "        markersize=50, markeredgewidth=2.5)\n",
    "textbox = ax.text(xs[0], -dIsdxs[1]-4,\n",
    "                  s='smooth gradients with\\nthe network interpolant', size=18,\n",
    "                  bbox={'facecolor': 'lightgray',\n",
    "                        'edgecolor': 'black',\n",
    "                        'alpha': 1,\n",
    "                        'pad': 5})\n",
    "ax.annotate('', xy=(xs[2], dIsdxs[5]-2),\n",
    "            xytext=(xs[11], -dIsdxs[2]+6),\n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.annotate('', xy=(xs[-6], dIsdxs[-2]),\n",
    "            xytext=(xs[11]+0.0013, -dIsdxs[2]-2.5), \n",
    "            arrowprops={'facecolor': 'black',})\n",
    "ax.set(xlabel=xlabel, ylabel=r'$\\mathrm{d}I$/$\\mathrm{d}x$ [A/m]',\n",
    "       xticks=xticks,\n",
    "       xticklabels=xticklabels)\n",
    "ax.legend(prop={'size': 18})\n",
    "\n",
    "# fname = os.path.join('figures', 'grad_nn_current')\n",
    "# save_fig(fig, fname=fname, formats=['png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815428d-6134-4e1e-af18-69a24793fd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
