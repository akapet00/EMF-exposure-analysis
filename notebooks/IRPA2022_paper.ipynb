{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "if os.getcwd().split(\"/\")[-1] == \"notebooks\":\n",
    "    os.chdir(os.pardir)\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as nn\n",
    "from jax import vmap, jit, grad\n",
    "from jax.experimental import optimizers\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "from src.constants import eps_0, mu_0, c, pi\n",
    "from src.utils.viz import fig_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'platform: {jax.lib.xla_bridge.get_backend().platform}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update('jax_enable_x64', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='ticks', palette='colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "Z_0 = np.sqrt(mu_0 / eps_0)  # free space impedance\n",
    "a = 2e-3  # radius of the wire\n",
    "N_elem = 61  # number of elements of the antenna\n",
    "L = 1  # antenna length\n",
    "dx = L / N_elem  # distance between the two elements of the antenna\n",
    "x = np.arange(0, L+dx/2, dx)  # spatial coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-michael",
   "metadata": {},
   "source": [
    "`sct_gnd_025_current.str` - holds the current distribution along the straight thin wire scatterer of a finite length of 1 m and the radius of 2 mm located at the height of 0.25 m above a lossy half-space (σ = 10 mS/m, ε = 10) that is illuminated by a normally incident electromagnetic field operating on 150 and 300 MHz, respectively.\n",
    "\n",
    "The first column represents spatial points on the antenna. The second column holds operating frequencies, while the third and forth columns are real and imaginary current parts, respectively. Finally, the fifth column is the absolute current value. The data were obtained by solving the Pocklington equation numerically by means of indirect boundary element method with the in-house software - `SuzANA` ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.loadtxt(os.path.join('data', 'source-model', 'dipole', 'sct_gnd_025_current.str'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load current over a half-wave dipole antenna for 0.3 GHz frequency\n",
    "\n",
    "f = I[N_elem+1:, 1][0]\n",
    "omega = 2 * pi * f\n",
    "k = omega * np.sqrt(mu_0 * eps_0)\n",
    "I_an = I[N_elem+1:, 2] + 1j * I[N_elem+1:, 3]\n",
    "y = I_an * 1000  # in mA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x, np.abs(y), 'b-', lw=3, label='$|I(x)|$')\n",
    "ax.plot(x, np.real(y), 'r-', lw=3, label='$\\Re{[I(x)]}$')\n",
    "ax.plot(x, np.imag(y), 'g-', lw=3, label='$\\Im{[I(x)]}$')\n",
    "ax.set(xlabel='$x$ [m]', ylabel='$I$ [mA]')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-recognition",
   "metadata": {},
   "source": [
    "# Finite-step derivative approximation on interpolated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diff(fn, h=1e-6):\n",
    "    \"\"\"Forward difference y = np.abs(I_an) * 1000  # in mAapproximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x + h) - fn(x)) / h\n",
    "    return dfn_dx\n",
    "    \n",
    "    \n",
    "def backward_diff(fn, h=1e-6):\n",
    "    \"\"\"Backward difference approximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x) - fn(x - h)) / h\n",
    "    return dfn_dx\n",
    "\n",
    "    \n",
    "def central_diff(fn, h=1e-6):\n",
    "    \"\"\"Central difference approximation.\"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return (fn(x + h) - fn(x - h)) / (2 * h)\n",
    "    return dfn_dx\n",
    "\n",
    "\n",
    "def complex_step_diff(fn, h=1e-6):\n",
    "    \"\"\"Complex-step difference approximation.\n",
    "    \n",
    "    Note: Incompatible with SciPy interpolation module.\n",
    "    \"\"\"\n",
    "    def dfn_dx(x):\n",
    "        return np.imag(fn(x + 1j * h)) / h\n",
    "    return dfn_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadratic interpolation of current distribution function\n",
    "\n",
    "f = interp1d(x, np.abs(y), kind='quadratic')\n",
    "\n",
    "x_new = np.linspace(x.min(), x.max(), 1001)\n",
    "y_new = f(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-comedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean relative percentage error\n",
    "\n",
    "error = np.mean(np.abs(np.abs(y)[1:-1] - f(x)[1:-1]) / np.abs(y)[1:-1]) * 100\n",
    "print(f'relative error = {error:.2e} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-consciousness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution as calculated and interpolated\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x, np.abs(y), 'b-', lw=4, label='computed')\n",
    "ax.plot(x_new, y_new, 'r--', lw=4, label='interpolated')\n",
    "ax.set(xlabel='$x$ [m]', ylabel='$I$ [mA]')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finite difference on computed and interpolated data\n",
    "\n",
    "dfdx = np.r_[(forward_diff(f)(x[0]), central_diff(f)(x[1:-1]), backward_diff(f)(x[-1]))]\n",
    "dfdx_new = np.r_[(forward_diff(f)(x_new[0]), central_diff(f)(x_new[1:-1]), backward_diff(f)(x_new[-1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize current gradient distribution as calculated and interpolated\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x, dfdx, 'b-', lw=4, label='computed')\n",
    "ax.plot(x_new, dfdx_new, 'r--', lw=4, label='interpolated')\n",
    "ax.plot([x[2], x[-2]], [dfdx[2], dfdx[-2]], 'ko', fillstyle='none', markersize=40, markeredgewidth=2.5)\n",
    "ax.annotate('', xy=(x[2], dfdx[2] - 0.5), xytext=(x[11], -dfdx[2] - 2), \n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.annotate('', xy=(x[-5], dfdx[-2]), xytext=(x[11], -dfdx[2] - 2), \n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.text(x[0], -dfdx[0] - 2, 'numeric artifacts',\n",
    "        bbox={'facecolor': 'wheat',\n",
    "              'edgecolor': 'black',\n",
    "              'alpha': 0.5,\n",
    "              'pad': 5})\n",
    "ax.set(xlabel='$x$ [m]', ylabel=r'$\\mathrm{d}I$/$\\mathrm{d}x$ [mA/m]')\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-plain",
   "metadata": {},
   "source": [
    "# Automatic differentiation on neural network-based interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = jnp.asarray(x_new).reshape(-1, 1)\n",
    "y_train = jnp.asarray(y_new).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"Initialize network parameters.\"\"\"\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    def random_layer_params(m, n, key, scale=1e-2):\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        return (scale * jax.random.normal(w_key, (n, m)),\n",
    "                scale * jax.random.normal(b_key, (n, )))\n",
    "    return [random_layer_params(m, n, key)\n",
    "            for m, n, key in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "\n",
    "def forward(params, X):\n",
    "    \"\"\"Forward pass.\"\"\"\n",
    "    output = X\n",
    "    for w, b in params[:-1]:\n",
    "        output = nn.tanh(w @ output + b)\n",
    "    w, b = params[-1]\n",
    "    output = w @ output + b\n",
    "    return output\n",
    "\n",
    "\n",
    "# vectorized mapping of network input, `X`, on `forward` function\n",
    "batch_forward = vmap(forward, in_axes=(None, 0))\n",
    "\n",
    "\n",
    "@jit\n",
    "def loss_fn(params, batch):\n",
    "    \"\"\"Summed square error loss function.\"\"\"\n",
    "    X, y = batch\n",
    "    y_pred = batch_forward(params, X)\n",
    "    return jnp.sum(jnp.square(y_pred - y))\n",
    "\n",
    "\n",
    "# derivative of the loss function\n",
    "grad_fn = jit(grad(loss_fn))\n",
    "\n",
    "\n",
    "@jit\n",
    "def update(step, optim_state, batch):\n",
    "    \"\"\"Return current optimal state of the network.\"\"\"\n",
    "    params = optim_params(optim_state)\n",
    "    grads = grad_fn(params, batch)\n",
    "    optim_state = optim_update(step, grads, optim_state)\n",
    "    return optim_state\n",
    "\n",
    "\n",
    "def data_stream(num_train, num_batches):\n",
    "    \"\"\"Training data random generator.\"\"\"\n",
    "    rng = npr.RandomState(0)\n",
    "    while True:\n",
    "        perm = rng.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "            yield x_train[batch_idx], y_train[batch_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set network hyperparameter and train\n",
    "\n",
    "step_size = 1e-3\n",
    "n_epochs = 25_000\n",
    "printout = int(n_epochs / 100.)\n",
    "epochs = np.arange(0, n_epochs+1, step=printout)\n",
    "batch_size = 64\n",
    "momentum_mass = 0.9  # for momentum and adagrad\n",
    "sizes = [1, 128, 256, 128, 1]\n",
    "\n",
    "num_train = x_train.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "batches = data_stream(num_train, num_batches)\n",
    "\n",
    "optim_init, optim_update, optim_params = optimizers.adam(step_size)\n",
    "init_params = init_network_params(sizes, rng)\n",
    "optim_state = optim_init(init_params)\n",
    "itercount = itertools.count()\n",
    "\n",
    "loss_train = []\n",
    "start_time = time.time()\n",
    "for epoch in trange(n_epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    for _ in range(num_batches):\n",
    "        optim_state = update(next(itercount), optim_state, next(batches))\n",
    "    epoch_duration = time.time() - start_epoch_time\n",
    "    \n",
    "    params = optim_params(optim_state)\n",
    "    if (epoch == 0) or (epoch % printout == (printout - 1)):\n",
    "        loss_train.append(loss_fn(params, (x_train, y_train)))\n",
    "training_duration = time.time() - start_time\n",
    "print(f'Training time: {training_duration:.2f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training loss dynamics\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(epochs, loss_train, 'b-', lw=4)\n",
    "ax.set(xlabel='epoch', ylabel='loss', yscale='log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize spatial current distribution as calculated and fitted\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x_train.flatten(), y_train.flatten(), 'b-', lw=4, label='computed')\n",
    "ax.plot(x_train.flatten(), batch_forward(params, x_train).flatten(), 'r--', lw=4, label='fitted')\n",
    "ax.set(xlabel='$x$ [m]', ylabel='$I$ [mA]')\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def I_an_nn(x):\n",
    "    \"\"\"Current value at specific location, `x`.\n",
    "    \n",
    "    Note: This is single-value wrapper for the forward pass function.\n",
    "    \"\"\"\n",
    "    return forward(params, x)[0]\n",
    "\n",
    "\n",
    "# derivative of the current approximation function\n",
    "grad_I_an_nn = jit(vmap(grad(I_an_nn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize current gradient distribution as calculated and fitted\n",
    "\n",
    "fig_config(latex=True, scaler=1.5)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(x_new, dfdx_new, 'b-', lw=4, label='computed')\n",
    "ax.plot(x_train, grad_I_an_nn(x_train), 'r--', lw=4, label='fitted')\n",
    "ax.plot([x[2], x[-2]], [dfdx[2], dfdx[-2]], 'ko', fillstyle='none', markersize=40, markeredgewidth=2.5)\n",
    "ax.annotate('', xy=(x[2], dfdx[2] - 0.5), xytext=(x[11], -dfdx[2] - 1), \n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.annotate('', xy=(x[-5], dfdx[-2]), xytext=(x[11], -dfdx[2] - 1), \n",
    "            arrowprops={'facecolor': 'black'})\n",
    "ax.text(x[0], -dfdx[0] - 2, fontsize=15,\n",
    "        s='smooth gradients on\\nedge elements with\\nthe network interpolant',        \n",
    "        bbox={'facecolor': 'wheat',\n",
    "              'edgecolor': 'black',\n",
    "              'alpha': 0.5,\n",
    "              'pad': 5})\n",
    "ax.set(xlabel='$x$ [m]', ylabel=r'$\\mathrm{d}I$ / $\\mathrm{d}x$ [mA/m]')\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-satellite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
